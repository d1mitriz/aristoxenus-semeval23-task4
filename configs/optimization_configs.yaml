data:
  train_dataset_path: data/splits/arguments-training.tsv
  train_labels_path: data/splits/labels-training.tsv
  val_dataset_path: data/splits/arguments-val.tsv
  val_labels_path: data/splits/labels-val.tsv
tapt: True
model:
  pretrained_name: 'Roberta_TAPT'
  pretrained: models/TAPT/roberta-base/checkpoint-219
  n_classes: 20
  # dropout: 0.2 Left out for optimization
  pooling: True
  normalization: False
  clipping: True
  # model_folder: models Left out to create new folder per optuna run
  # model_name: miniLM.pt Left out to store new model per optuna run
training:
  ova: True
  seed: 13
  downsample: True
  # lr: 0.0001 Left out for optimization
  # batch_size: 16 Left out for optimization
  epochs: 100
  threshold: 0.35 # If the predicted score is higher than it then this label is assigned to this instance
  max_len: 128
  beta1: 0.9
  beta2: 0.999
  weight_decay: 1e-3
  # warmup_steps: 10 Left out for optimization
  early_stopping_patience: 20 # set to 0 to ignore early stopping callback
optuna:
  n_trials: 100
  n_jobs: 1
tracking:
  tracking_uri: "SET_CUSTOM_TRACKING_URI"
  experiment_name: "EXPERIMENT_NAME"
against_phrases:
  - " so it is not valid to say that "
  - " so it is wrong that "
in_favor_phrases:
  - " so "
  - " thus "
  - " therefore "
  - ". Subsequently "
  - ". As a result "
  - ". So it is valid to say that "
  - ", so it is true that "
